Key-value pairs are important to the MapReduce model. Input is presented as key-value pairs. And, output is generated as a key-value pair. Interestingly enough, this convention makes it easy to form multi-stage processing by directly using the output from one stage as the input to another stage.
Keys are usually simple. But, the values associated with them can be large and complex. For example, consider a URL and the Web page it describes as a key-value pair. The value, the contents of the page, can be complex and information rich.
Unsurprisingly the MapReduce model is based on a pair of functions: Map and Reduce. The Map function begin with a single -input- key-value pair and produces one or more -intermediate- key-value pairs. There may be, and often are, more than one intermediate pair given a single input pair. And, the key is not necessarily unique within the intermediate pairs -- there can be, and often are, duplicates.
And, that's exactly where the Reduce operation comes in. Reduce takes a key and a -list- of all associated values and reduces that list to a smaller list -- typically of size -zero- or -one-. In other words, typical Reduce functions will produce a single value, or no value -- but producing a list of more than one value is, in practice, not often useful.
The Reduce function is also executed in parallel, resulting in more than output set of key-value pairs. These can be further processed by Map operations, which don't necessarily need for them to be in one partition, further combined via Reduces, or given back to the User program, which might well be able to handle the results spread out across some number of files.
So, there you have it. The Map function, which is performed independently, in a massively parallel way, on separate partions of the data, does a bunch of user-defined processing, and structures its output as key-value pairs. These pairs are then aggregated by the Reduce function, and processed some more or handed back to the user.
